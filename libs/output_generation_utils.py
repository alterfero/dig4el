from __future__ import annotations
import json
from docx import Document
from libs import knowledge_graph_utils as kgu
from libs import utils
from libs import stats
from io import BytesIO
import re
from collections import defaultdict
import pandas as pd
import graphviz
from html import escape
from io import BytesIO
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils import get_column_letter
import time
from io import BytesIO
from typing import Any, Dict, Union, Optional

from openpyxl import load_workbook

import jinja2

DEFAULT_DELIMITERS = [" ", ".", ",", ";", ":", "!", "?", "…"]

# ======================= IMPORT CQ XLS. HERE BECAUSE EXPORT IS HERE =================================

def _norm(s: Any) -> str:
    """Best-effort normalize to a clean string."""
    if s is None:
        return ""
    s = str(s)
    # preserve user content; just trim
    return s.strip()


def _read_info_kv(ws) -> Dict[str, str]:
    """
    Reads Info sheet key/value pairs that were written as:
      A5..: key, B5..: value
    """
    kv: Dict[str, str] = {}
    for row in ws.iter_rows(min_row=1, max_col=2):
        k = _norm(row[0].value)
        v = _norm(row[1].value) if len(row) > 1 else ""
        if k:
            kv[k] = v
    return kv


def _extract_uid_from_info(ws_info) -> str:
    """
    Builder wrote:
      A3 = f"“UID: {cq.get('uid', '')}”"
    so parse whatever is after UID:
    """
    a3 = _norm(ws_info["A3"].value)
    if not a3:
        return ""
    return a3[4:]


def cq_translation_from_transcription_xlsx(
    xlsx: Union[str, bytes, BytesIO],
    *,
    default_target_language: str = "target language",
) -> Dict[str, Any]:
    """
    Parse a completed CQ transcription workbook (generated by generate_transcription_xlsx)
    and return a CQ translation JSON dict.

    Parameters
    ----------
    xlsx:
        Path to .xlsx OR bytes OR BytesIO.

    default_target_language:
        Used if the Info sheet didn't specify a target language.
    """
    if isinstance(xlsx, (bytes, bytearray)):
        xlsx = BytesIO(xlsx)

    wb = load_workbook(xlsx, data_only=True)

    if "Info" not in wb.sheetnames:
        raise ValueError("Missing required sheet: 'Info'")
    if "Transcription" not in wb.sheetnames:
        raise ValueError("Missing required sheet: 'Transcription'")

    ws_info = wb["Info"]
    ws_t = wb["Transcription"]

    info_kv = _read_info_kv(ws_info)

    target_language = _norm(info_kv.get("Target language", "")) or default_target_language
    pivot_language = _norm(info_kv.get("Pivot language (if any)", "")) or "English"

    transcription_made_by = _norm(info_kv.get("Transcription made by", ""))
    provided_by_key = f"Content in {target_language} provided by"
    content_provided_by = _norm(info_kv.get(provided_by_key, ""))

    location = _norm(info_kv.get("Location", "")) or "unknown"

    cq_uid = _extract_uid_from_info(ws_info)

    # ---- Parse Transcription sheet ----
    # Header in row 1:
    # ["Index","English","Pivot (if not English)",f"Sentence in {target_language}",
    #  "Literal English Back-Translation","Concept item","Word(s) contributing...","Notes"]
    data: Dict[str, Any] = {}

    current_index: Optional[str] = None

    # Iterate rows after header
    for r in ws_t.iter_rows(min_row=2, values_only=True):
        idx = _norm(r[0])
        english = _norm(r[1])
        pivot = _norm(r[2])
        target_sentence = _norm(r[3])
        lebt = _norm(r[4])
        concept_item = _norm(r[5])
        concept_words = _norm(r[6])
        notes = _norm(r[7])

        if idx:
            current_index = idx
        #
        # if not current_index:
        #     # no index yet => ignore stray rows
        #     continue

        if current_index not in data:
            data[current_index] = {
                "legacy index": current_index,
                "cq": english,
                "alternate_pivot": pivot,
                "translation": target_sentence,
                "concept_words": {},
                "lebt": "",
                "comment": ""
            }

        entry = data[current_index]

        # Update turn-level fields if present on this row
        if english:
            entry["cq"] = english
        if pivot:
            entry["alternate_pivot"] = pivot
        if target_sentence:
            entry["translation"] = target_sentence

        if lebt:
            entry["lebt"] = lebt
        if notes:
            entry["comment"] = notes

        # Concept mapping row
        if concept_item:
            # concept_words in the form word1...word2...etc
            if concept_words:
                print("idx: {}".format(idx))
                print("entry: {}".format(entry))
                print("translation: {}".format(entry["translation"]))
                print("concept_item: {}".format(concept_item))
                print("concept_words: {}".format(concept_words))
                # cleaning and validation (valid if in the target sentence)
                words_in_sentence = stats.custom_split(entry["translation"])
                print("words_in_sentence: {}".format(words_in_sentence))
                wcl_tmp = concept_words.split("...")
                wcl = [w.lower().strip() for w in wcl_tmp]
                validated_wcl = [w for w in wcl if w in words_in_sentence]
                print("validated_wcl: {}".format(validated_wcl))
                validated_concept_words = "...".join(validated_wcl)
                entry["concept_words"][concept_item] = validated_concept_words
                print("validated_concept_words: {}".format(validated_concept_words))
                print("final entry: {}".format(entry))
            else:
                entry["concept_words"][concept_item] = ""


    # ---- Build final JSON ----
    interviewer = transcription_made_by or ""
    interviewee = content_provided_by or ""

    out = {
        "target language": target_language,
        "delimiters": DEFAULT_DELIMITERS,
        "pivot language": pivot_language,
        "cq_uid": cq_uid,
        "data": data,
        "interviewer": interviewer,
        "interviewee": interviewee,
        "recording_uid": str(int(time.time())),
        "owner name": " and ".join([x for x in [interviewee, interviewer] if x]) or "",
        "owner orcid": "",
        "authorization": "accessed read-only by anyone via DIG4EL tools",
        "location": location,
    }

    return out

# ======================================================================================

def generate_transcription_doc(cq, target_language, pivot_language):
    cq = utils.normalize_user_strings(cq)
    target_language = utils.normalize_text(target_language)
    pivot_language = utils.normalize_text(pivot_language)
    if target_language in ["English", ""]:
        target_language = "target language"

    document = Document()
    document.add_heading(safe_text('Conversational Questionnaire'), 0)
    document.add_heading(safe_text(f'"{cq["title"]}"'), 0)
    document.add_heading(safe_text("Information"), 1)
    table = document.add_table(rows=5, cols=2, style="Light Grid")  # Fixed column count
    row_cells0 = table.rows[0].cells
    row_cells0[0].text = safe_text("Target language")
    if target_language != "target language":
        row_cells0[1].text = safe_text(target_language)
    row_cells1 = table.rows[1].cells
    row_cells1[0].text = safe_text("Transcription made by")
    row_cells2 = table.rows[2].cells
    row_cells2[0].text = safe_text(f"Content in {target_language} provided by")
    row_cells3 = table.rows[3].cells
    row_cells3[0].text = safe_text("Location")
    row_cells4 = table.rows[4].cells
    row_cells4[0].text = safe_text("Date")

    document.add_page_break()

    document.add_heading(safe_text("Full dialog in English"), 1)
    document.add_paragraph(safe_text(cq["context"]))
    dialog_table = document.add_table(rows=1, cols=3, style="Table Grid")  # Fixed column count
    hdr_cells = dialog_table.rows[0].cells
    hdr_cells[0].text = safe_text("Index")
    hdr_cells[1].text = safe_text(cq["speakers"]["A"]["name"])
    hdr_cells[2].text = safe_text(cq["speakers"]["B"]["name"])
    dialog_length = len(cq["dialog"])
    for index in range(1, dialog_length + 1):
        content = cq["dialog"][str(index)]
        row_cells = dialog_table.add_row().cells
        if content.get("legacy index", "") != "":
            i = content["legacy index"]
        else:
            i = str(index)
        row_cells[0].text = safe_text(i)
        if content["speaker"] == "A":
            row_cells[1].text = safe_text(content["text"])
            row_cells[2].text = safe_text("")
        elif content["speaker"] == "B":
            row_cells[1].text = safe_text("")
            row_cells[2].text = safe_text(content["text"])

    document.add_page_break()

    document.add_heading(safe_text("Transcription"), 1)

    for index in range(1, dialog_length + 1):
        content = cq["dialog"][str(index)]
        if content.get("legacy index", "") != "":
            i = content["legacy index"]
        else:
            i = str(index)
        document.add_paragraph(safe_text(""))
        document.add_heading(safe_text(f'{i}: {content["text"]}'), 2)
        transcription_table = document.add_table(rows=0, cols=2, style="Table Grid")
        row_cells = transcription_table.add_row().cells
        row_cells[0].text = safe_text("Pivot (if not English)")
        row_cells[1].text = safe_text("")
        row_cells = transcription_table.add_row().cells
        row_cells[0].text = safe_text(f'Sentence in {target_language}')
        row_cells[1].text = safe_text("")
        document.add_paragraph(safe_text(" "))
        p = document.add_paragraph()
        p.add_run(safe_text("Connections between word(s) and concept(s):")).bold = True
        document.add_paragraph(safe_text("In this segment, which word(s) contribute to which concept(s) below? One word can appear in multiple concepts, or in none. Multiple words can appear in a single concept."))
        concept_table = document.add_table(rows=1, cols=2, style="Table Grid")
        hdr_cells = concept_table.rows[0].cells
        hdr_cells[0].text = safe_text("Concept")
        hdr_cells[1].text = safe_text("Word(s) contributing to this concept")
        if content.get("intent", []) != []:
            row_cells = concept_table.add_row().cells
            row_cells[0].text = safe_text(f'Intent: {"+".join(content["intent"])}')
            row_cells[1].text = safe_text("")
        if content.get("predicate", []) != []:
            row_cells = concept_table.add_row().cells
            row_cells[0].text = safe_text(f'Type of predicate: {"+".join(content["predicate"])}')
            row_cells[1].text = safe_text("")
        for c in content["concept"]:
            row_cells = concept_table.add_row().cells
            row_cells[0].text = safe_text(c)
            row_cells[1].text = safe_text("")

    # Save to a BytesIO buffer instead of disk
    docx_buffer = BytesIO()
    document.save(docx_buffer)
    docx_buffer.seek(0)  # Reset buffer position

    return docx_buffer



def generate_transcription_xlsx(cq, target_language, pivot_language):
    """
    Create an Excel workbook template for CQ transcription/annotation.

    Sheets:
      - Info: questionnaire metadata + contributor fields
      - Dialog: the English dialog, one row per turn
      - Transcription: normalized "long" format, one row per (turn, concept_item)
          concept_item includes: Intent:..., Type of predicate:..., and each concept in content["concept"].
    Returns: BytesIO positioned at 0.
    """
    cq = utils.normalize_user_strings(cq)
    target_language = utils.normalize_text(target_language)
    pivot_language = utils.normalize_text(pivot_language)

    if target_language in ["English", ""]:
        target_language = "target language"

    wb = Workbook()

    # ---------- helpers ----------
    header_fill = PatternFill("solid", fgColor="EDEDED")
    bold = Font(bold=True)
    wrap = Alignment(wrap_text=True, vertical="top")
    top = Alignment(vertical="top")

    def set_col_width(ws, widths):
        for col_idx, width in enumerate(widths, start=1):
            ws.column_dimensions[get_column_letter(col_idx)].width = width

    def make_header(ws, row_idx, headers):
        for c, h in enumerate(headers, start=1):
            cell = ws.cell(row=row_idx, column=c, value=h)
            cell.font = bold
            cell.fill = header_fill
            cell.alignment = Alignment(wrap_text=True, vertical="center")
        ws.freeze_panes = ws[f"A{row_idx+1}"]

    # ---------- Sheet 1: Info ----------
    ws_info = wb.active
    ws_info.title = "Info"

    ws_info["A1"] = "Conversational Questionnaire"
    ws_info["A1"].font = Font(bold=True, size=20)
    ws_info["A2"] = f"“{cq.get('title','')}”"
    ws_info["A2"].font = Font(bold=True, size=14)
    ws_info["A3"] = "UID:{}".format(cq.get('uid', ''))
    ws_info["A3"].font = Font(bold=True, size=14)

    rows = [
        ("Target language", "" if target_language == "target language" else target_language),
        ("Pivot language (if any)", "" if pivot_language in ["English", ""] else pivot_language),
        ("Transcription made by", ""),
        (f"Content in {target_language} provided by", ""),
        ("Location", ""),
        ("Date", "")
    ]
    start = 5
    ws_info[f"A{start-1}"] = "Information"
    ws_info[f"A{start-1}"].font = Font(bold=True, size=14)

    for i, (k, v) in enumerate(rows):
        r = start + i
        ws_info[f"A{r}"] = k
        ws_info[f"A{r}"].font = bold
        ws_info[f"A{r}"].alignment = top
        ws_info[f"B{r}"] = v
        ws_info[f"B{r}"].alignment = top

    ws_info["A12"] = "Full dialog context (English)"
    ws_info["A12"].font = bold
    ws_info["A13"] = cq.get("context", "")
    ws_info["A13"].alignment = wrap
    ws_info.merge_cells("A13:D18")

    set_col_width(ws_info, [34, 50, 18, 18])

    # ---------- Sheet 2: Dialog ----------
    ws_dialog = wb.create_sheet("Dialog")
    speaker_a = cq.get("speakers", {}).get("A", {}).get("name", "Speaker A")
    speaker_b = cq.get("speakers", {}).get("B", {}).get("name", "Speaker B")

    make_header(ws_dialog, 1, [
        "Index",
        "Speaker",
        speaker_a,
        speaker_b,
    ])

    dialog = cq.get("dialog", {})
    dialog_length = len(dialog)

    for idx in range(1, dialog_length + 1):
        content = dialog.get(str(idx), {})
        legacy = content.get("legacy index", "")
        shown_index = legacy if legacy != "" else str(idx)

        speaker = content.get("speaker", "")
        text = content.get("text", "")

        row = [
            shown_index,
            speaker,
            text if speaker == "A" else "",
            text if speaker == "B" else "",
        ]
        ws_dialog.append(row)

    # formatting
    set_col_width(ws_dialog, [10, 10, 60, 60])
    for row in ws_dialog.iter_rows(min_row=2):
        for cell in row:
            cell.alignment = wrap

    # ---------- Sheet 3: Transcription (long format) ----------
    ws_t = wb.create_sheet("Transcription")

    make_header(ws_t, 1, [
        "Index",
        "English",
        "Pivot (if not English)",
        f"Sentence in {target_language}",
        "Literal English Back-Translation",
        "Concept item",
        "Word(s) contributing to this concept item (separated by ... when multiple)",
        "Notes",
    ])

    for idx in range(1, dialog_length + 1):
        content = dialog.get(str(idx), {})
        legacy = content.get("legacy index", "")
        shown_index = legacy if legacy != "" else str(idx)

        english_turn = content.get("text", "")

        # Each "concept item" becomes its own row => easy to query later.
        concept_items = []

        if content.get("intent", []) != []:
            concept_items.append(f'Intent: {"+".join(content["intent"])}')

        for c in content.get("concept", []) or []:
            concept_items.append(str(c))

        # Always create at least one row per turn, even if no concept items exist.
        if not concept_items:
            concept_items = [""]
        first = True
        for concept_item in concept_items:
            if first:
                ws_t.append([
                    shown_index,
                    english_turn,
                    "",          # pivot
                    "",          # target sentence
                    "",          # LEBT
                    concept_item,
                    "",          # word(s) mapping
                    "",          # notes
                ])
                first = False
            else:
                ws_t.append([
                    "",  # don't repeat index
                    "",  # don't repeat English
                    "",  # pivot
                    "",  # target sentence
                    "",  # LEBT
                    concept_item,
                    "",  # word(s) mapping
                    "",  # notes
                ])

    set_col_width(ws_t, [10, 12, 45, 30, 45, 40, 45, 30])
    for row in ws_t.iter_rows(min_row=2):
        for cell in row:
            cell.alignment = wrap

    # ---------- Save to BytesIO ----------
    xlsx_buffer = BytesIO()
    wb.save(xlsx_buffer)
    xlsx_buffer.seek(0)
    return xlsx_buffer



def generate_docx_from_kg_index_list(kg, delimiters, kg_index_list):
    kg = utils.normalize_user_strings(kg)
    delimiters = utils.normalize_user_strings(delimiters)
    document = Document()
    document.add_heading(safe_text('Partial corpus'), 0)
    item_counter = 0
    for index in kg_index_list:
        item_counter += 1
        data = kg[index]
        gloss = kgu.build_super_gloss_df(kg, index, delimiters,  output_dict=True)
        document.add_paragraph(safe_text("""Entry {}""".format(item_counter)), style='List Bullet')
        pex = document.add_paragraph(safe_text(" "), "Normal")
        pex.add_run(safe_text(f'{data["recording_data"]["translation"]}'), style='Strong')
        document.add_paragraph(safe_text(f'{data["sentence_data"]["text"]}'), "Normal")
        document.add_paragraph(safe_text(f'''
        Intent: {", ".join(data["sentence_data"]["intent"])}
        Type of predicate: {", ".join(data["sentence_data"]["predicate"])}
        '''), "Normal")

        # Define the number of rows (one header + words) and columns
        num_words = len(gloss)
        table = document.add_table(rows=num_words + 1, cols=4, style="Light Shading Accent 1")  # Fixed column count

        # Populate header row
        hdr_cells = table.rows[0].cells
        hdr_cells[0].text = safe_text("")
        hdr_cells[1].text = safe_text("Concept")
        hdr_cells[2].text = safe_text("Internal Particularisation")
        hdr_cells[3].text = safe_text("Relational Particularisation")

        # Populate the table with data
        for i, w in enumerate(gloss, start=1):
            row_cells = table.rows[i].cells
            row_cells[0].text = safe_text(w["word"])
            row_cells[1].text = safe_text(w["concept"])
            row_cells[2].text = safe_text(w["internal particularization"])
            row_cells[3].text = safe_text(w["relational particularization"])
        document.add_paragraph(safe_text("""   """))

    # Save to a BytesIO buffer instead of disk
    docx_buffer = BytesIO()
    document.save(docx_buffer)
    docx_buffer.seek(0)  # Reset buffer position

    return docx_buffer

_MAX_WORD_COLS = 63             # Word hard‑limit on columns  :contentReference[oaicite:0]{index=0}
_BAD_XML_CHARS_RE = re.compile(r"[\x00-\x08\x0b\x0c\x0e-\x1f]")

def safe_text(value: object) -> str:
    """Return XML‑safe, non‑None, length‑capped string for a table cell."""
    if value is None or (isinstance(value, float) and pd.isna(value)):
        return ""
    text = str(value)
    # strip control chars Word’s XML parser can’t handle
    text = _BAD_XML_CHARS_RE.sub("", text)
    text = utils.normalize_text(text)
    # Word has no documented per‑cell limit, but 32 K keeps you well clear
    return text[:32760]

def add_dataframe_table(doc: Document,
                        df: pd.DataFrame,
                        style: str = "LightShading-Accent1") -> None:
    """
    Render *df* as a Word table and append it to *doc*, coping with:
      • empty frames
      • >63 columns
      • missing / localised style names
      • NaN / None / weird unicode in the data
    """
    if df.empty or df.shape[1] == 0:
        return                                      # nothing to do

    # Word refuses tables wider than 63 columns
    if df.shape[1] > _MAX_WORD_COLS:
        df = df.iloc[:, :_MAX_WORD_COLS]

    # build the table (first row = header row)
    table = doc.add_table(rows=1, cols=df.shape[1])

    # try the requested style first, fall back gracefully if Word/doc lacks it
    try:
        # API style names omit the hyphen: “Light Shading – Accent 1”
        # becomes “Light Shading Accent 1” :contentReference[oaicite:1]{index=1}
        table.style = style
    except KeyError:
        pass                                         # default style is OK

    # ---- header row -----------------------------------------------------
    for cell, col in zip(table.rows[0].cells, df.columns):
        cell.text = safe_text(col)

    # ---- data rows ------------------------------------------------------
    for _, row in df.iterrows():
        for cell, val in zip(table.add_row().cells, row):
            cell.text = safe_text(val)


STRIP_IDX_RE = re.compile(r'(_\d+|\(\d+\))$', re.VERBOSE)  # to remove indexes _1 or (1) added to target words
NONWORD_RE = re.compile(r'\W+')
WORD_COL_CANDIDATES = ("target_word", "word")

STRIP_IDX_RE = re.compile(r"(_\d+|\(\d+\))$")      # strips _2  or (2)


# ------------------------------------------------------------------
#  main helper
# ------------------------------------------------------------------
def _prepare_words(df: pd.DataFrame) -> pd.DataFrame:
    # 1) find the column that holds the word
    for col in WORD_COL_CANDIDATES:
        if col in df.columns:
            word_col = col
            break
    else:
        raise KeyError(
            f"None of {WORD_COL_CANDIDATES} found in DataFrame columns "
            f"{list(df.columns)}"
        )

    out = df.copy()

    # ------------------------------------------------------------------
    # Strip “_2” or “(2)” only once and reuse the result everywhere
    # ------------------------------------------------------------------
    clean_word = (
        out[word_col]
        .fillna("")
        .astype(str)
        .str.replace(r"(_\d+|\(\d+\))$", "", regex=True)
    )

    # ---- what the reader will see ------------------------------------
    out["display_word"] = clean_word

    # ---- Graphviz node‑id (slug of the cleaned word) ------------------
    out["word_node_id"] = (
        "word_" + clean_word.str.replace(r"\W+", "_", regex=True)
    )

    # ---- concept node‑id (unchanged) ----------------------------------
    out["concept_node_id"] = (
        "concept_" +
        (out["concept"].astype(str)
         + "|" + out["IP"].astype(str)
         + "|" + out["RP"].astype(str))
        .apply(hash).astype(str)
    )

    return out

# ------------------------------------------------------------------
#  main helper
# ------------------------------------------------------------------
def add_concept_graph(doc: Document, df: pd.DataFrame) -> None:
    """
    Build a left‑to‑right bipartite graph:
        target_word  ──▶  concept (IP, RP)
    and insert it as a picture into *doc*.

    Required df columns: target_word, concept, IP, RP
    """
    df = _prepare_words(df)          # ← use the cleaned copy

    g = graphviz.Digraph('WordConcept', format='png')
    g.attr(rankdir='LR', splines='true', overlap='false')

    # ---------- left column: target words ---------------------------
    with g.subgraph(name='cluster_targets') as s:
        s.attr(rank='same')
        for node_id, label in (df[['word_node_id', 'display_word']]
                               .drop_duplicates()
                               .sort_values('display_word')
                               .itertuples(index=False, name=None)):
            s.node(node_id, label,
                   shape='box', style='rounded,filled',
                   fillcolor='#f0f8ff', fontsize='14')

    # ---------- right column: concept + IP + RP ---------------------
    with g.subgraph(name='cluster_concepts') as s:
        s.attr(rank='same')
        for node_id, c, ip, rp in (
                df[["concept_node_id", "concept", "IP", "RP"]]
                        .drop_duplicates()
                        .sort_values(["concept", "IP", "RP"])
                        .itertuples(index=False, name=None)
        ):
            label = f"{c}\nIP={ip}  RP={rp}"
            s.node(node_id, label,
                   shape="ellipse", style="filled",
                   fillcolor="#ffffe0", fontsize="12")

    # ---------- edges -----------------------------------------------
    for _, r in df.iterrows():
        g.edge(r.word_node_id, r.concept_node_id)

    # ---------- render & insert -------------------------------------
    stream = BytesIO(g.pipe(format='png'))
    doc.add_picture(stream, width=doc.sections[0].page_width * 0.6)
    doc.add_paragraph()           # blank line after the picture


def generate_docx_from_hybrid_output(content, language, gloss_format="table"):
    document = Document()
    document.add_heading(safe_text('Learning {}'.format(language)), 0)
    if "introduction" in content.keys():
        document.add_heading(safe_text('Introduction'), 1)
        document.add_paragraph(safe_text(content["introduction"]["description"]))
    for chapter in content["chapters"]:
        document.add_heading(safe_text(chapter["title"]), 1)
        document.add_paragraph(safe_text(chapter["explanation"]))
        e_counter = 0
        for example in chapter["examples"]:
            e_counter += 1
            document.add_paragraph(safe_text("""Example {}""".format(e_counter)), style='List Bullet')
            pex = document.add_paragraph(safe_text(" "), "Normal")
            pex.add_run(safe_text(f'{example["target"]}'), style='Strong')
            document.add_paragraph(safe_text(f'{example["english"]}'), "Normal")

            df = parse_alterlingua(example["gloss"])

            if gloss_format == "table":
                add_dataframe_table(document, df)
            elif gloss_format == "graph":
                add_concept_graph(document, df)

            document.add_paragraph(safe_text("""   """))

    # Save to a BytesIO buffer instead of disk
    docx_buffer = BytesIO()
    document.save(docx_buffer)
    docx_buffer.seek(0)  # Reset buffer position

    return docx_buffer

def generate_plain_language_docx_from_hybrid_output(content, language):
    document = Document()
    document.add_heading(safe_text('Learning {}'.format(language)), 0)
    if "introduction" in content.keys():
        document.add_heading(safe_text('Introduction'), 1)
        document.add_paragraph(safe_text(content["introduction"]["description"]))
    for chapter in content["chapters"]:
        document.add_heading(safe_text(chapter["title"]), 1)
        document.add_paragraph(safe_text(chapter["explanation"]))
        e_counter = 0
        for example in chapter["examples"]:
            e_counter += 1
            document.add_paragraph(safe_text("""Example {}""".format(e_counter)), style='List Bullet')
            pex = document.add_paragraph(safe_text(" "), "Normal")
            pex.add_run(safe_text(f'{example["target"]}'), style='Strong')
            document.add_paragraph(safe_text(f'{example["english"]}'), "Normal")
            document.add_paragraph(safe_text("""   """))
            document.add_paragraph(safe_text(f'{example["gloss"]}'), "Normal")
            document.add_paragraph(safe_text("""   """))

    # Save to a BytesIO buffer instead of disk
    docx_buffer = BytesIO()
    document.save(docx_buffer)
    docx_buffer.seek(0)  # Reset buffer position

    return docx_buffer

def generate_lesson_docx_from_aggregated_output(content, indi_language, readers_language):
    loc = {
        "English": {
            "introduction": "Introduction",
            "for example": "For example",
            "in conclusion": "In Conclusion",
            "more_examples": "More examples",
            "learning_the_x_language_grammar": "Learning the X language grammar"
        },
        "Bislama": {
            "introduction": "Introdaksen",
            "for example": "Foa eksampol",
            "in conclusion": "Long fin",
            "more_examples": "Mo eksampol",
            "learning_the_x_language_grammar": "Lernim graema blong lanwis X"
        },
        "French": {
            "introduction": "Introduction",
            "for example": "Par exemple",
            "in conclusion": "En conclusion",
            "more_examples": "Plus d'exemples",
            "learning_the_x_language_grammar": "Apprentissage de la grammaire de la langue X"
        },
        "Japanese": {
            "introduction": "はじめに",
            "for example": "例えば",
            "in conclusion": "結論として",
            "more_examples": "さらに例",
            "learning_the_x_language_grammar": "X言語の文法を学ぶ"
        },
        "Swedish": {
            "introduction": "Introduktion",
            "for example": "Till exempel",
            "in conclusion": "Sammanfattningsvis",
            "more_examples": "Fler exempel",
            "learning_the_x_language_grammar": "Att lära sig X-språkets grammatik"
        }
    }
    locit = loc.get(readers_language, "English")
    learning_X = locit["learning_the_x_language_grammar"].replace("X", indi_language)

    document = Document()

    document.add_paragraph("Generated by DIG4EL " + content.get("date", "unknown") + ", DIG4EL version " + content.get("version", "version unknonwn"))

    document.add_heading(safe_text(content["title"]), level=0)
    document.add_paragraph(safe_text("  "))
    document.add_paragraph(safe_text(content["introduction"]))

    for section in content["sections"]:
        document.add_heading(safe_text(section["focus"]), level=2)

        if isinstance(section["description"], str):
            document.add_paragraph(section["description"])
        elif isinstance(section["description"], dict):
            document.add_paragraph(section["description"].get("description", "..."))

        document.add_paragraph(safe_text(locit["for example"] + ": "))

        if isinstance(section.get("example"), list):
            e_count = 0
            for example in section.get("example"):
                e_count += 1
                pex = document.add_paragraph(safe_text(" "), "Normal")
                ct = str(e_count)
                pex.add_run(safe_text(f'{ct}) '))
                tt = safe_text(example["target_sentence"])
                pex.add_run(safe_text(f'{tt}'), style='Strong')
                tm = safe_text(example["source_sentence"])
                document.add_paragraph(safe_text(f'{tm}'), "Normal")
                document.add_paragraph(safe_text(example["description"]))
        if isinstance(section.get("example"), dict):
            example = section["example"]
            pex = document.add_paragraph(safe_text(" "), "Normal")
            tt = safe_text(example["target_sentence"])
            pex.add_run(safe_text(f'{tt}'), style='Strong')
            tm = safe_text(example["source_sentence"])
            document.add_paragraph(safe_text(f'({tm})'), "Normal")
            document.add_paragraph(safe_text(example["description"]))



    document.add_heading(safe_text(locit["in conclusion"]), level=1)
    document.add_paragraph(safe_text(" "))
    document.add_paragraph(safe_text(content["conclusion"]))

    document.add_heading(safe_text(locit["more_examples"] + ": "))
    document.add_paragraph(safe_text(" "))
    for i, s in enumerate(content["translation_drills"]):
        aex = document.add_paragraph(safe_text(" "), style="List Bullet")
        tma = safe_text(s["target"])
        aex.add_run(safe_text(f'{tma}'), style='Strong')
        document.add_paragraph(safe_text(f'({s["source"]})'), "Normal")

    document.add_heading(safe_text("Sources"), level=1)
    sources = content["sources"]
    if "documents" in sources.keys():
        document.add_heading(safe_text("Documents: "), level=2)
        for d in sources["documents"]:
            document.add_paragraph(safe_text(d), style="List Bullet")
    if "cqs" in sources.keys():
        document.add_heading(safe_text("Conversational Questionnaires: "), level=2)
        for d in sources["cqs"]:
            document.add_paragraph(safe_text(d), style="List Bullet")
    if "pairs" in sources.keys():
        document.add_heading(safe_text("Sentence pairs: "), level=2)
        for d in sources["pairs"]:
            document.add_paragraph(safe_text(d), style="List Bullet")
    # Save to a BytesIO buffer instead of disk
    docx_buffer = BytesIO()
    document.save(docx_buffer)
    docx_buffer.seek(0)  # Reset buffer position

    return docx_buffer


def generate_docx_from_grammar_json(grammar_json, language):
    h1_index = 0
    h2_index = 0
    document = Document()
    document.add_heading(safe_text('{}: Elements of grammar.'.format(language)), 0)
    for topic, content in grammar_json.items():
        h1_index += 1
        document.add_heading(safe_text(f"{str(h1_index)}. {topic}"), 1)
        for parameter, description in content.items():
            h2_index += 1
            document.add_heading(safe_text(f"{str(h1_index)}.{str(h2_index)}. {topic}: {parameter}"), 2)
            intro_text = f"""
            In {language}, {parameter} is mainly {description["main value"]}
            """
            p = document.add_paragraph(safe_text(intro_text))
            for value, examples in description["examples by value"].items():
                document.add_heading(safe_text(f"""Example of {value}: """), 3)
                example_counter = 0
                for example in examples:
                    example_counter += 1
                    document.add_paragraph(safe_text("""Example {}""".format(example_counter)), style='List Bullet')
                    pex = document.add_paragraph(safe_text(" "), "Normal")
                    pex.add_run(safe_text(f'{example["translation"]}'), style='Strong')
                    document.add_paragraph(safe_text(f'{example["english sentence"]}'), "Normal")

                    # Define the number of rows (one header + words) and columns
                    num_words = len(example["gloss"])
                    table = document.add_table(rows=num_words + 1, cols=4, style="Light Shading Accent 1")  # Fixed column count

                    # Populate header row
                    hdr_cells = table.rows[0].cells
                    hdr_cells[0].text = safe_text(language)
                    hdr_cells[1].text = safe_text("Concept")
                    hdr_cells[2].text = safe_text("Internal Particularisation")
                    hdr_cells[3].text = safe_text("Relational Particularisation")

                    # Populate the table with data
                    for i, (w, g) in enumerate(example["gloss"].items(), start=1):
                        row_cells = table.rows[i].cells
                        row_cells[0].text = safe_text(w)
                        row_cells[1].text = safe_text(g["concept"])
                        row_cells[2].text = safe_text(g["internal particularization"])
                        row_cells[3].text = safe_text(g["relational particularization"])
                    document.add_paragraph(safe_text("""   """))

        # Save to a BytesIO buffer instead of disk
        docx_buffer = BytesIO()
        document.save(docx_buffer)
        docx_buffer.seek(0)  # Reset buffer position
        return docx_buffer
    return None


def parse_alterlingua(text):
    # Regex to find all target word blocks
    word_blocks = re.findall(r'(\S+)<([^>]*)>', text)

    results = []

    for target_word, content in word_blocks:
        if not content.strip():
            continue  # skip empty concept blocks

        # Split multiple concepts joined by '&'
        concepts = [c.strip() for c in content.split('&')]

        for idx, concept in enumerate(concepts, start=1):
            # Regex to extract the concept name and its IP / RP fields
            concept_match = re.match(r'([^()]+)\(([^)]*)\)', concept)
            if concept_match:
                concept_name, fields = concept_match.groups()
                concept_name = concept_name.strip()

                # Extract IP and RP using regex or string methods
                ip_match = re.search(r'IP:\s*([^|]*)', fields)
                rp_match = re.search(r'RP:\s*([^|]*)', fields)
                ip = ip_match.group(1).strip() if ip_match else ""
                rp = rp_match.group(1).strip() if rp_match else ""

                entry = {
                    "target_word": f"{target_word}({idx})" if len(concepts) > 1 else target_word,
                    "concept": concept_name,
                    "IP": ip,
                    "RP": rp
                }
                results.append(entry)

    return pd.DataFrame(results)


